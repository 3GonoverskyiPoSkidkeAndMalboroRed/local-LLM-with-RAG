<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Документация local-LLM-with-RAG</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-top: 30px;
        }
        h2 {
            color: #3498db;
            margin-top: 25px;
        }
        code {
            background-color: #f5f5f5;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        pre {
            background-color: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            line-height: 1.4;
        }
        .tree {
            font-family: 'Courier New', Courier, monospace;
            white-space: pre;
            margin-left: 20px;
        }
        ul {
            margin-left: 20px;
        }
        .tech-stack {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 15px;
        }
        .tech-item {
            background-color: #e1f5fe;
            padding: 5px 10px;
            border-radius: 5px;
            font-weight: 500;
        }
        .command {
            background-color: #282c34;
            color: #abb2bf;
            padding: 10px 15px;
            border-radius: 5px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <h1>Документация для проекта local-LLM-with-RAG</h1>

    <h1>Участники разработки</h1>
    <p>В разработке принимали участие П. А. Максиво и П. А. Неверов из отдела "Лаборатория инструментов искусственного интеллекта" компании НПО "СПЕКТРОН".</p>

    <h2>Структура проекта</h2>
    <p>Проект представляет собой веб-приложение для работы с локальными языковыми моделями (LLM) с использованием технологии Retrieval-Augmented Generation (RAG). Основной интерфейс построен на базе Streamlit, а бэкенд реализован с использованием FastAPI.</p>

    <h2>Алгоритмическое древо</h2>
    <div class="tree">local-LLM-with-RAG
├── main_page.py         # Основная точка входа в Streamlit-приложение
├── app.py               # FastAPI бэкенд
├── llm.py               # Обработка запросов языковой модели
├── document_loader.py   # Загрузка и индексация документов
├── models.py            # Работа с моделями Ollama
├── pages/               # Страницы Streamlit-приложения
│   ├── user_guide.py    # Руководство пользователя
│   ├── ui_client.py     # Режим консультанта (RAG-режим)
│   ├── generate.py      # Режим генерации текста
│   └── admin.py         # Административная панель
└── Research/            # Директория с документами для базы знаний</div>

    <h1>Описание работы каждого файла</h1>

    <h2>main_page.py</h2>
    <p>Главная точка входа в Streamlit-приложение. Определяет структуру навигации между различными страницами приложения:</p>
    <ul>
        <li>Руководство пользователя</li>
        <li>Режим консультанта</li>
        <li>Генерация</li>
        <li>Админка</li>
    </ul>
    <pre><code>import streamlit as st
# Определяем страницы
page_0 = st.Page("pages/user_guide.py", title="Руководство пользователя")
page_1 = st.Page("pages/ui_client.py", title="Режим консультанта")
page_2 = st.Page("pages/generate.py", title="Генерация")
page_3 = st.Page("pages/admin.py", title="Админка")
# Настраиваем навигацию
pg = st.navigation([page_0, page_1, page_2, page_3])
# Запускаем выбранную страницу
pg.run()</code></pre>

    <h2>app.py</h2>
    <p>Серверная часть приложения, реализованная с использованием FastAPI. Предоставляет API-эндпоинты для:</p>
    <ul>
        <li>Генерации текста (/generate)</li>
        <li>Запросов к базе знаний с использованием RAG (/query)</li>
        <li>Инициализации языковой модели (/initialize)</li>
        <li>Загрузки файлов (/upload-file)</li>
    </ul>
    <p>Поддерживает два режима работы: веб-сервер и консольный интерфейс.</p>

    <h2>llm.py</h2>
    <p>Содержит логику для работы с языковыми моделями:</p>
    <ul>
        <li>Определяет шаблоны промптов для обработки вопросов</li>
        <li>Реализует функцию getChatChain, которая создает цепочку обработки запросов с использованием RAG</li>
        <li>Обеспечивает контекстное понимание моделью истории диалога</li>
    </ul>

    <h2>document_loader.py</h2>
    <p>Отвечает за загрузку и обработку документов:</p>
    <ul>
        <li>Загружает документы различных форматов (PDF, Markdown) из указанной директории</li>
        <li>Разбивает документы на части (чанки) для эффективного поиска</li>
        <li>Создает векторные представления документов и сохраняет их в базе данных Chroma</li>
        <li>Реализует функцию vec_search для семантического поиска релевантных фрагментов по запросу</li>
    </ul>

    <h2>models.py</h2>
    <p>Управляет работой с моделями Ollama:</p>
    <ul>
        <li>Проверяет наличие моделей локально</li>
        <li>Загружает модели при необходимости</li>
        <li>Предоставляет список доступных моделей</li>
    </ul>

    <h2>pages/user_guide.py</h2>
    <p>Руководство пользователя, отображающее информацию о том, как использовать приложение:</p>
    <ul>
        <li>Отображает PDF с руководством</li>
        <li>Содержит описание и скриншоты различных страниц приложения</li>
    </ul>

    <h2>pages/ui_client.py</h2>
    <p>Интерфейс режима консультанта с использованием RAG:</p>
    <ul>
        <li>Отображает чат-интерфейс для взаимодействия с пользователем</li>
        <li>Отправляет запросы к API для получения ответов на основе базы знаний</li>
        <li>Сохраняет историю диалога</li>
    </ul>

    <h2>pages/generate.py</h2>
    <p>Интерфейс для генерации текста без использования RAG:</p>
    <ul>
        <li>Позволяет пользователю вводить текст для генерации</li>
        <li>Отправляет запросы к API для генерации текста</li>
        <li>Отображает историю генераций</li>
    </ul>

    <h2>pages/admin.py</h2>
    <p>Административная панель:</p>
    <ul>
        <li>Защищена паролем</li>
        <li>Позволяет загружать файлы в базу знаний</li>
        <li>Предоставляет возможность инициализировать модель с выбранными параметрами</li>
        <li>Позволяет отправлять тестовые запросы к модели</li>
    </ul>

    <h1>Процесс работы</h1>
    
    <h2>Инициализация:</h2>
    <ul>
        <li>Пользователь запускает приложение через Streamlit</li>
        <li>В фоне запускается FastAPI сервер для обработки запросов</li>
    </ul>

    <h2>Режим консультанта (RAG):</h2>
    <ul>
        <li>Пользователь задает вопрос в интерфейсе</li>
        <li>Запрос отправляется на сервер</li>
        <li>Сервер выполняет семантический поиск релевантных фрагментов текста</li>
        <li>Фрагменты добавляются к запросу и отправляются в языковую модель</li>
        <li>Результат возвращается пользователю</li>
    </ul>

    <h2>Режим генерации:</h2>
    <ul>
        <li>Пользователь вводит текст для генерации</li>
        <li>Запрос отправляется напрямую в языковую модель без RAG</li>
        <li>Результат возвращается пользователю</li>
    </ul>

    <h2>Админка:</h2>
    <ul>
        <li>Администратор может загружать новые документы</li>
        <li>Реиндексировать базу знаний</li>
        <li>Тестировать работу системы</li>
    </ul>

    <h1>Технологический стек</h1>
    <div class="tech-stack">
        <div class="tech-item">Frontend: Streamlit</div>
        <div class="tech-item">Backend: FastAPI</div>
        <div class="tech-item">Модели: Ollama (локальный запуск моделей)</div>
        <div class="tech-item">Векторная база данных: Chroma</div>
        <div class="tech-item">Обработка документов: LangChain, PyPDF2</div>
        <div class="tech-item">Контейнеризация: Docker</div>
    </div>

    <h1>Запуск приложения</h1>
    <p>Приложение может быть запущено как локально, так и с использованием Docker.</p>
    
    <h2>Локальный запуск</h2>
    <div class="command">streamlit run main_page.py</div>
    <div class="command">docker-compose up -d</div>
    <div class="command">python app.py --model &lt;название_модели&gt; --embedding_model &lt;название_модели_эмбеддингов&gt; --path &lt;путь_к_документам&gt; --web --port &lt;порт&gt;</div>

    <p>Дополнительная информация доступна в <a href="Readme.md">Readme.md</a> и <a href="readme_docker.md">readme_docker.md</a>.</p>
</body>
</html> 