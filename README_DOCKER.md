# Запуск приложения локальной LLM с RAG в Docker

Это руководство описывает, как запустить приложение для работы с локальными моделями LLM и RAG в Docker.

## Требования

- [Docker](https://www.docker.com/products/docker-desktop) - установленный Docker
- [Ollama](https://ollama.ai/) - установленный на основной машине
- Загруженные модели LLM через Ollama (например, `ollama pull mistral`)

## Шаги для запуска

### 1. Сборка Docker образа

```bash
docker build -t local-llm-rag .
```

### 2. Запуск контейнера

#### Вариант 1: Через docker run

```bash
docker run -d -p 8000:8000 -p 8501:8501 -v ./Research:/app/Research --add-host=host.docker.internal:host-gateway local-llm-rag
```

#### Вариант 2: Через docker-compose (рекомендуется)

```bash
docker-compose up -d
```

### 3. Доступ к приложению

После запуска приложения доступны два интерфейса:

- **API (FastAPI)**: http://localhost:8000/docs
- **Веб-интерфейс (Streamlit)**: http://localhost:8501

### 4. Доступ по сети

Для доступа с других устройств в локальной сети используйте IP-адрес вашего компьютера вместо localhost:

- **API**: http://<IP-адрес-компьютера>:8000/docs
- **Веб-интерфейс**: http://<IP-адрес-компьютера>:8501

## Примечания

- Убедитесь, что Ollama запущен на основной машине перед запуском Docker контейнера
- Docker контейнер соединяется с Ollama на основной машине через host.docker.internal
- Папка `Research` монтируется в контейнер, чтобы документы были доступны приложению

## Устранение неполадок

- **Проблема**: Не удается подключиться к Ollama
  **Решение**: Убедитесь, что Ollama запущен на основной машине и доступен для подключения

- **Проблема**: Не доступен интерфейс по сети
  **Решение**: Проверьте настройки брандмауэра и разрешите входящие соединения на порты 8000 и 8501 