Документация для проекта local-LLM-with-RAG <Заголвок 1 порядка>

Структура проекта <Заголвок 2 порядка>
Проект представляет собой веб-приложение для работы с локальными языковыми моделями (LLM) с использованием технологии Retrieval-Augmented 
Generation (RAG). Основной интерфейс построен на базе Streamlit, а бэкенд реализован с использованием FastAPI.


Алгоритмическое древо <Заголвок 2 порядка>
local-LLM-with-RAG
├── main_page.py         # Основная точка входа в Streamlit-приложение
├── app.py               # FastAPI бэкенд
├── llm.py               # Обработка запросов языковой модели
├── document_loader.py   # Загрузка и индексация документов
├── models.py            # Работа с моделями Ollama
├── pages/               # Страницы Streamlit-приложения
│   ├── user_guide.py    # Руководство пользователя
│   ├── ui_client.py     # Режим консультанта (RAG-режим)
│   ├── generate.py      # Режим генерации текста
│   └── admin.py         # Административная панель
└── Research/            # Директория с документами для базы знаний


Описание работы каждого файла <Заголвок 1 порядка>

main_page.py <Заголвок 2 порядка>

Главная точка входа в Streamlit-приложение. Определяет структуру навигации между различными страницами приложения:
Руководство пользователя
Режим консультанта
Генерация
Админка

import streamlit as st
# Определяем страницы
page_0 = st.Page("pages/user_guide.py", title="Руководство пользователя")
page_1 = st.Page("pages/ui_client.py", title="Режим консультанта")
page_2 = st.Page("pages/generate.py", title="Генерация")
page_3 = st.Page("pages/admin.py", title="Админка")
# Настраиваем навигацию
pg = st.navigation([page_0, page_1, page_2, page_3])
# Запускаем выбранную страницу
pg.run()


app.py <Заголвок 2 порядка>
Серверная часть приложения, реализованная с использованием FastAPI. Предоставляет API-эндпоинты для:
Генерации текста (/generate)
Запросов к базе знаний с использованием RAG (/query)
Инициализации языковой модели (/initialize)
Загрузки файлов (/upload-file)
Поддерживает два режима работы: веб-сервер и консольный интерфейс.

llm.py <Заголвок 2 порядка>
Содержит логику для работы с языковыми моделями:
Определяет шаблоны промптов для обработки вопросов
Реализует функцию getChatChain, которая создает цепочку обработки запросов с использованием RAG
Обеспечивает контекстное понимание моделью истории диалога

document_loader.py <Заголвок 2 порядка>
Отвечает за загрузку и обработку документов:
Загружает документы различных форматов (PDF, Markdown) из указанной директории
Разбивает документы на части (чанки) для эффективного поиска
Создает векторные представления документов и сохраняет их в базе данных Chroma
Реализует функцию vec_search для семантического поиска релевантных фрагментов по запросу

models.py <Заголвок 2 порядка>
Управляет работой с моделями Ollama:
Проверяет наличие моделей локально
Загружает модели при необходимости
Предоставляет список доступных моделей

pages/user_guide.py <Заголвок 2 порядка>
Руководство пользователя, отображающее информацию о том, как использовать приложение:
Отображает PDF с руководством
Содержит описание и скриншоты различных страниц приложения
pages/ui_client.py <Заголвок 2 порядка>
Интерфейс режима консультанта с использованием RAG:
Отображает чат-интерфейс для взаимодействия с пользователем
Отправляет запросы к API для получения ответов на основе базы знаний
Сохраняет историю диалога
pages/generate.py <Заголвок 2 порядка>
Интерфейс для генерации текста без использования RAG:
Позволяет пользователю вводить текст для генерации
Отправляет запросы к API для генерации текста
Отображает историю генераций
pages/admin.py <Заголвок 2 порядка>
Административная панель:
Защищена паролем
Позволяет загружать файлы в базу знаний
Предоставляет возможность инициализировать модель с выбранными параметрами
Позволяет отправлять тестовые запросы к модели


Процесс работы <Заголвок 1 порядка>
Инициализация: <Заголвок 2 порядка>
Пользователь запускает приложение через Streamlit
В фоне запускается FastAPI сервер для обработки запросов

Режим консультанта (RAG): <Заголвок 2 порядка>
Пользователь задает вопрос в интерфейсе
Запрос отправляется на сервер
Сервер выполняет семантический поиск релевантных фрагментов текста
Фрагменты добавляются к запросу и отправляются в языковую модель
Результат возвращается пользователю

Режим генерации: <Заголвок 2 порядка>
Пользователь вводит текст для генерации
Запрос отправляется напрямую в языковую модель без RAG
Результат возвращается пользователю

Админка: <Заголвок 2 порядка>
Администратор может загружать новые документы
Реиндексировать базу знаний
Тестировать работу системы

Технологический стек <Заголвок 1 порядка>
Frontend: Streamlit
Backend: FastAPI
Модели: Ollama (локальный запуск моделей)
Векторная база данных: Chroma
Обработка документов: LangChain, PyPDF2
Контейнеризация: Docker

Запуск приложения <Заголвок 1 порядка>
Приложение может быть запущено как локально, так и с использованием Docker.
Локальный запуск <Заголвок 2 порядка>
streamlit run main_page.py
docker-compose up -d
python app.py --model <название_модели> --embedding_model <название_модели_эмбеддингов> --path <путь_к_документам> --web --port <порт>