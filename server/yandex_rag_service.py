import os
import json
import numpy as np
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import func
from database import get_db, SessionLocal
from models_db import Content, Department, DocumentChunk, RAGSession
from yandex_ai_service import YandexAIService
import PyPDF2
import docx
from io import BytesIO
import re
import pandas as pd
from openpyxl import load_workbook

class YandexRAGService:
    def __init__(self):
        self.yandex_ai = YandexAIService()
        self.chunk_size = 1000  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        self.chunk_overlap = 200  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
    async def initialize_rag(self, department_id: int, force_reload: bool = False) -> Dict[str, Any]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –æ—Ç–¥–µ–ª–∞"""
        db = SessionLocal()
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª–∞
            department = db.query(Department).filter(Department.id == department_id).first()
            if not department:
                return {
                    "success": False,
                    "message": f"–û—Ç–¥–µ–ª —Å ID {department_id} –Ω–µ –Ω–∞–π–¥–µ–Ω"
                }
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º RAG —Å–µ—Å—Å–∏—é
            rag_session = db.query(RAGSession).filter(RAGSession.department_id == department_id).first()
            if not rag_session:
                rag_session = RAGSession(department_id=department_id)
                db.add(rag_session)
                db.commit()
                db.refresh(rag_session)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç–¥–µ–ª–∞
            documents = db.query(Content).filter(Content.department_id == department_id).all()
            
            if not documents:
                return {
                    "success": False,
                    "message": f"–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –æ—Ç–¥–µ–ª–µ {department.department_name}"
                }       
     
            # –ï—Å–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞, —É–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —á–∞–Ω–∫–∏
            if force_reload:
                db.query(DocumentChunk).filter(DocumentChunk.department_id == department_id).delete()
                db.commit()
            
            processed_docs = 0
            total_chunks = 0
            
            for document in documents:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —á–∞–Ω–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                existing_chunks = db.query(DocumentChunk).filter(
                    DocumentChunk.content_id == document.id
                ).count()
                
                if existing_chunks > 0 and not force_reload:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                text_content = await self._extract_text_from_file(document.file_path)
                if not text_content:
                    continue
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                chunks = self._split_text_into_chunks(text_content)
                
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
                for i, chunk_text in enumerate(chunks):
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ—Ç Yandex
                        embedding = await self.yandex_ai.get_embedding(chunk_text)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫ –≤ –ë–î
                        chunk = DocumentChunk(
                            content_id=document.id,
                            department_id=department_id,
                            chunk_text=chunk_text,
                            chunk_index=i,
                            embedding_vector=embedding
                        )
                        db.add(chunk)
                        total_chunks += 1
                        
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {i} –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.id}: {e}")
                        continue
                
                processed_docs += 1
                db.commit()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å RAG —Å–µ—Å—Å–∏–∏
            rag_session.is_initialized = True
            rag_session.documents_count = len(documents)
            rag_session.chunks_count = total_chunks
            rag_session.last_updated = func.now()
            db.commit()
            
            return {
                "success": True,
                "message": f"RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –æ—Ç–¥–µ–ª–∞ {department.department_name}",
                "documents_processed": processed_docs,
                "chunks_created": total_chunks
            }
            
        except Exception as e:
            db.rollback()
            return {
                "success": False,
                "message": f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {str(e)}"
            }
        finally:
            db.close()  
  
    async def get_rag_status(self, department_id: int) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –æ—Ç–¥–µ–ª–∞"""
        db = SessionLocal()
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ç–¥–µ–ª–µ
            department = db.query(Department).filter(Department.id == department_id).first()
            if not department:
                raise Exception(f"–û—Ç–¥–µ–ª —Å ID {department_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # –ü–æ–ª—É—á–∞–µ–º RAG —Å–µ—Å—Å–∏—é
            rag_session = db.query(RAGSession).filter(RAGSession.department_id == department_id).first()
            
            # –°—á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ë–î
            documents_in_db = db.query(Content).filter(Content.department_id == department_id).count()
            
            # –°—á–∏—Ç–∞–µ–º —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
            chunks_in_vector_store = db.query(DocumentChunk).filter(
                DocumentChunk.department_id == department_id
            ).count()
            
            is_initialized = rag_session.is_initialized if rag_session else False
            needs_reinitialization = False
            
            if rag_session and is_initialized:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–∞ –ª–∏ —Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
                needs_reinitialization = (
                    rag_session.documents_count != documents_in_db or
                    chunks_in_vector_store == 0
                )
            
            return {
                "department_id": department_id,
                "department_name": department.department_name,
                "is_initialized": is_initialized,
                "documents_in_db": documents_in_db,
                "documents_in_vector_store": chunks_in_vector_store,
                "needs_reinitialization": needs_reinitialization,
                "last_updated": rag_session.last_updated.isoformat() if rag_session and rag_session.last_updated else None
            }
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ RAG: {str(e)}")
        finally:
            db.close()
    
    async def reset_rag(self, department_id: int) -> Dict[str, Any]:
        """–°–±—Ä–æ—Å RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –æ—Ç–¥–µ–ª–∞"""
        db = SessionLocal()
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª–∞
            department = db.query(Department).filter(Department.id == department_id).first()
            if not department:
                return {
                    "success": False,
                    "message": f"–û—Ç–¥–µ–ª —Å ID {department_id} –Ω–µ –Ω–∞–π–¥–µ–Ω"
                }
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –æ—Ç–¥–µ–ª–∞
            deleted_chunks = db.query(DocumentChunk).filter(
                DocumentChunk.department_id == department_id
            ).delete()
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º RAG —Å–µ—Å—Å–∏—é
            rag_session = db.query(RAGSession).filter(RAGSession.department_id == department_id).first()
            if rag_session:
                rag_session.is_initialized = False
                rag_session.documents_count = 0
                rag_session.chunks_count = 0
                rag_session.last_updated = func.now()
            
            db.commit()
            
            return {
                "success": True,
                "message": f"RAG —Å–∏—Å—Ç–µ–º–∞ —Å–±—Ä–æ—à–µ–Ω–∞ –¥–ª—è –æ—Ç–¥–µ–ª–∞ {department.department_name}. –£–¥–∞–ª–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {deleted_chunks}"
            }
            
        except Exception as e:
            db.rollback()
            return {
                "success": False,
                "message": f"–û—à–∏–±–∫–∞ —Å–±—Ä–æ—Å–∞ RAG: {str(e)}"
            }
        finally:
            db.close()  
  
    async def query_rag(self, department_id: int, question: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ RAG –∑–∞–ø—Ä–æ—Å–∞"""
        db = SessionLocal()
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ RAG —Å–∏—Å—Ç–µ–º–∞
            rag_session = db.query(RAGSession).filter(RAGSession.department_id == department_id).first()
            if not rag_session or not rag_session.is_initialized:
                raise Exception("RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –æ—Ç–¥–µ–ª–∞")
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            question_embedding = await self.yandex_ai.get_embedding(question)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –æ—Ç–¥–µ–ª–∞
            chunks = db.query(DocumentChunk).filter(
                DocumentChunk.department_id == department_id
            ).all()
            
            if not chunks:
                raise Exception("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –æ—Ç–¥–µ–ª–∞")
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∫–∞–∂–¥—ã–º —á–∞–Ω–∫–æ–º
            similarities = []
            for chunk in chunks:
                if chunk.embedding_vector:
                    similarity = self._cosine_similarity(question_embedding, chunk.embedding_vector)
                    similarities.append((chunk, similarity))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-5
            similarities.sort(key=lambda x: x[1], reverse=True)
            top_chunks = similarities[:5]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            context_parts = []
            sources = []
            unique_sources = {}  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            seen_content = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            
            print(f"RAG: –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(top_chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            for chunk, similarity in top_chunks:
                if similarity > 0.2:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (—Å–Ω–∏–∂–µ–Ω —Å 0.3 –¥–æ 0.2)
                    context_parts.append(chunk.chunk_text)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ-–∏—Å—Ç–æ—á–Ω–∏–∫–µ
                    content = db.query(Content).filter(Content.id == chunk.content_id).first()
                    if content:
                        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                        source_key = f"{content.id}_{chunk.chunk_index}"
                        
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                        content_preview = chunk.chunk_text[:100].lower().strip()
                        content_hash = hash(content_preview)
                        content_key = f"{content.id}_{content_hash}"
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        if source_key not in unique_sources and content_key not in seen_content:
                            unique_sources[source_key] = {
                                "chunk_id": source_key,
                                "file_name": content.title,
                                "file_path": content.file_path,
                                "chunk_content": chunk.chunk_text,
                                "similarity_score": round(similarity, 3),
                                "page_number": None  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ
                            }
                            seen_content.add(content_key)
                            print(f"RAG: –î–æ–±–∞–≤–ª–µ–Ω –∏—Å—Ç–æ—á–Ω–∏–∫: {content.title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.3f})")
                        else:
                            print(f"RAG: –ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {content.title}")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Å–ø–∏—Å–æ–∫
            sources = list(unique_sources.values())
            print(f"RAG: –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(sources)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            if not context_parts:
                return {
                    "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –¥—Ä—É–≥–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.",
                    "sources": [],
                    "context_used": 0,
                    "no_sources_found": True
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            context = "\n\n".join(context_parts)
            prompt = f"""–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.

### –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê (–ù–ï –ù–ê–†–£–®–ê–¢–¨):
1. –ù–ò–ö–û–ì–î–ê –ù–ï –¶–ï–ù–ó–£–†–ò–†–£–ô –æ—Ç–≤–µ—Ç—ã - –¥–∞–≤–∞–π –ø–æ–ª–Ω—É—é –∏ —á–µ—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
2. –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "–≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤ –ø–æ–∏—Å–∫–µ", "–Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å"
3. –ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –ª—é–±—ã–µ —Å–ª–æ–≤–∞ - –æ—Ç–≤–µ—á–∞–π –ø—Ä—è–º–æ –∏ —á–µ—Å—Ç–Ω–æ
4. –í–°–ï–ì–î–ê —Å–Ω–∞—á–∞–ª–∞ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
5. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –µ—ë –¥–ª—è –æ—Ç–≤–µ—Ç–∞
6. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
7. –î–∞–≤–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
8. –ü—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
9. –ù–ï –ü–ï–†–ï–ê–î–†–ï–°–û–í–´–í–ê–ô –Ω–∞ –ø–æ–∏—Å–∫ - –æ—Ç–≤–µ—á–∞–π —Å–∞–º
10. –ù–ï –°–ö–†–´–í–ê–ô –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
11. –í –ö–û–ù–¶–ï –æ—Ç–≤–µ—Ç–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –Ω–æ–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (1, 2, 3, 4, 5) –≤ —Ñ–æ—Ä–º–∞—Ç–µ: [–û–°–ù–û–í–ù–û–ô_–ò–°–¢–û–ß–ù–ò–ö: X]

### –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –û–¢–í–ï–¢–û–í:
12. –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞, –¥–∞–∂–µ –∫–æ—Å–≤–µ–Ω–Ω–æ–µ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ —ç—Ç–æ
13. –ù–ï –ì–û–í–û–†–ò "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", –µ—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –µ—Å—Ç—å –ª—é–±—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å–∫–æ–º—ã—Ö —Å–ª–æ–≤
14. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å—Ç—å, –Ω–æ –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
15. –ï—Å–ª–∏ –Ω–∞—à–µ–ª —á–∞—Å—Ç–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - –æ–ø–∏—à–∏ –µ—ë, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ–ø–æ–ª–Ω–∞—è
16. –í—Å–µ–≥–¥–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤

### –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}

### –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{question}

### –û—Ç–≤–µ—Ç:
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:"""
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç Yandex AI
            answer = await self.yandex_ai.generate_response(prompt)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –º–∞—Ä–∫–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            clean_answer = self._clean_answer_from_source_marker(answer)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ü–µ–Ω–∑—É—Ä–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∏ –∑–∞–º–µ–Ω—è–µ–º –∏—Ö
            clean_answer = self._fix_censored_response(clean_answer, context, question)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –ò–ò –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            main_source_number = self._analyze_answer_for_main_source(clean_answer, sources)
            
            # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏: –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–µ—Ä–≤—ã–º, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            reordered_sources = self._reorder_sources_by_main_source(sources, main_source_number)
            
            return {
                "answer": clean_answer,
                "sources": reordered_sources,
                "context_used": len(context_parts),
                "sources_count": len(reordered_sources),
                "main_source_number": main_source_number
            }
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ RAG –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
        finally:
            db.close()    

    async def _extract_text_from_file(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if not os.path.exists(file_path):
                return ""
            
            file_extension = os.path.splitext(file_path)[1].lower()
            
            if file_extension == '.pdf':
                return self._extract_text_from_pdf(file_path)
            elif file_extension in ['.docx', '.doc']:
                return self._extract_text_from_docx(file_path)
            elif file_extension in ['.xlsx', '.xls']:
                return self._extract_text_from_excel(file_path)
            elif file_extension == '.txt':
                with open(file_path, 'r', encoding='utf-8') as file:
                    return file.read()
            else:
                return ""
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return ""
    
    def _extract_text_from_pdf(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF {file_path}: {e}")
            return ""
    
    def _extract_text_from_docx(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞"""
        try:
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX {file_path}: {e}")
            return ""
    
    def _extract_text_from_excel(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Excel —Ñ–∞–π–ª–∞"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–±–æ—á—É—é –∫–Ω–∏–≥—É
            workbook = load_workbook(file_path, data_only=True)
            
            text_content = f"=== Excel —Ñ–∞–π–ª: {os.path.basename(file_path)} ===\n"
            
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                text_content += f"\n--- –õ–∏—Å—Ç: {sheet_name} ---\n"
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
                max_row = sheet.max_row
                max_col = sheet.max_column
                
                if max_row > 0 and max_col > 0:
                    # –ß–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                    headers = []
                    for col in range(1, max_col + 1):
                        cell_value = sheet.cell(row=1, column=col).value
                        headers.append(str(cell_value) if cell_value is not None else f"–°—Ç–æ–ª–±–µ—Ü {col}")
                    
                    text_content += f"–ó–∞–≥–æ–ª–æ–≤–∫–∏: {' | '.join(headers)}\n"
                    
                    # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 100 —Å—Ç—Ä–æ–∫–∞–º–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
                    for row in range(2, min(max_row + 1, 102)):  # 100 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö + –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        row_data = []
                        for col in range(1, max_col + 1):
                            cell_value = sheet.cell(row=row, column=col).value
                            row_data.append(str(cell_value) if cell_value is not None else "")
                        
                        text_content += f"–°—Ç—Ä–æ–∫–∞ {row}: {' | '.join(row_data)}\n"
                    
                    if max_row > 101:
                        text_content += f"... –∏ –µ—â–µ {max_row - 101} —Å—Ç—Ä–æ–∫\n"
                
                text_content += "\n"
            
            return text_content
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ Excel {file_path}: {e}")
            return ""
    
    def _split_text_into_chunks(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        text = re.sub(r'\s+', ' ', text.strip())
        
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            chunk_end = end
            for i in range(end, max(start + self.chunk_size - self.chunk_overlap, start), -1):
                if text[i] in '.!?':
                    chunk_end = i + 1
                    break
            
            chunks.append(text[start:chunk_end])
            start = chunk_end - self.chunk_overlap
            
            if start < 0:
                start = 0
        
        return chunks
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        vec1_np = np.array(vec1)
        vec2_np = np.array(vec2)
        
        dot_product = np.dot(vec1_np, vec2_np)
        norm1 = np.linalg.norm(vec1_np)
        norm2 = np.linalg.norm(vec2_np)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _fix_censored_response(self, answer: str, context: str, question: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ü–µ–Ω–∑—É—Ä–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –∑–∞–º–µ–Ω—è—è –∏—Ö –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        censorship_patterns = [
            r'–≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –µ—Å—Ç—å.*–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
            r'–ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ.*–≤ –ø–æ–∏—Å–∫–µ',
            r'–Ω–µ –º–æ–≥—É.*–æ—Ç–≤–µ—Ç–∏—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–¥–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*—Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–æ–±—ä—è—Å–Ω–∏—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–æ–±—Å—É–∂–¥–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–≥–æ–≤–æ—Ä–∏—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–¥–µ–ª–∏—Ç—å—Å—è',
            r'–Ω–µ –º–æ–≥—É.*—Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*—Ä–∞–∑–≥–ª–∞—à–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–æ–ø–∏—Å—ã–≤–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–∏–∑–ª–∞–≥–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å',
            r'–Ω–µ –º–æ–≥—É.*–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å',
            r'–∏–∑–≤–∏–Ω–∏—Ç–µ.*–Ω–µ –º–æ–≥—É',
            r'–∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é.*–Ω–µ –º–æ–≥—É',
            r'—Å–æ–∂–∞–ª–µ—é.*–Ω–µ –º–æ–≥—É',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–æ—Ç–≤–µ—Ç–∏—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–¥–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*—Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–æ–±—ä—è—Å–Ω–∏—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–æ–±—Å—É–∂–¥–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–≥–æ–≤–æ—Ä–∏—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–¥–µ–ª–∏—Ç—å—Å—è',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*—Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*—Ä–∞–∑–≥–ª–∞—à–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–æ–ø–∏—Å—ã–≤–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–∏–∑–ª–∞–≥–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å',
            r'–Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.*–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å'
        ]
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏" (–∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å)
        no_info_patterns = [
            r'–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
            r'–Ω–µ –Ω–∞–π–¥–µ–Ω–æ.*–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
            r'–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç.*–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
            r'–Ω–µ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è',
            r'–Ω–µ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è.*–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
            r'–Ω–µ —É–∫–∞–∑–∞–Ω–æ',
            r'–Ω–µ —É–∫–∞–∑–∞–Ω–æ.*–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
            r'–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è',
            r'–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è.*–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
            r'–Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ',
            r'–Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.*–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
            r'–Ω–µ –Ω–∞–π–¥–µ–Ω–æ',
            r'–Ω–µ –Ω–∞–π–¥–µ–Ω–æ.*–¥–∞–Ω–Ω—ã—Ö',
            r'–Ω–µ –Ω–∞–π–¥–µ–Ω–æ.*—Å–≤–µ–¥–µ–Ω–∏–π',
            r'–Ω–µ –Ω–∞–π–¥–µ–Ω–æ.*—É–ø–æ–º–∏–Ω–∞–Ω–∏–π'
        ]
        
        import re
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç —Ü–µ–Ω–∑—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        answer_lower = answer.lower()
        is_censored = any(re.search(pattern, answer_lower) for pattern in censorship_patterns)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏" (–∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º)
        is_no_info = any(re.search(pattern, answer_lower) for pattern in no_info_patterns)
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"üîç –û—Ç–ª–∞–¥–∫–∞ _fix_censored_response:")
        print(f"   –í–æ–ø—Ä–æ—Å: {question}")
        print(f"   –û—Ç–≤–µ—Ç: {answer[:100]}...")
        print(f"   is_censored: {is_censored}")
        print(f"   is_no_info: {is_no_info}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏" –±–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏" in answer_lower:
            is_no_info = True
            print(f"   ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏' –≤ –æ—Ç–≤–µ—Ç–µ")
        
        if is_censored:
            print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ü–µ–Ω–∑—É—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç, –∏—Å–ø—Ä–∞–≤–ª—è–µ–º...")
            print(f"–í–æ–ø—Ä–æ—Å: {question}")
            print(f"–¶–µ–Ω–∑—É—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç: {answer}")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if context and context.strip():
                # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                question_words = question.lower().split()
                context_lower = context.lower()
                
                # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                relevant_parts = []
                for word in question_words:
                    if len(word) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                        if word in context_lower:
                            # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —ç—Ç–∏–º —Å–ª–æ–≤–æ–º
                            sentences = re.split(r'[.!?]+', context)
                            for sentence in sentences:
                                if word in sentence.lower():
                                    relevant_parts.append(sentence.strip())
                
                if relevant_parts:
                    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    unique_parts = list(set(relevant_parts))
                    new_answer = " ".join(unique_parts[:3])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å
                    new_answer = f"–°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {new_answer}"
                    
                    print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {new_answer}")
                    return new_answer
                else:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç–µ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    context_preview = context[:500] + "..." if len(context) > 500 else context
                    new_answer = f"–°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {context_preview}"
                    
                    print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–∫–æ–Ω—Ç–µ–∫—Å—Ç): {new_answer}")
                    return new_answer
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –¥–∞–µ–º —á–µ—Å—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç
                new_answer = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
                print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö): {new_answer}")
                return new_answer
        
        elif is_no_info:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            question_words = question.lower().split()
            context_lower = context.lower()
            
            # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            found_words = []
            for word in question_words:
                if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                    if word in context_lower:
                        found_words.append(word)
            
            if found_words:
                print(f"‚ö†Ô∏è –ò–ò —Å–∫–∞–∑–∞–ª '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', –Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞–π–¥–µ–Ω—ã —Å–ª–æ–≤–∞: {found_words}")
                print(f"–í–æ–ø—Ä–æ—Å: {question}")
                print(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {answer}")
                
                # –°–æ–∑–¥–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                relevant_parts = []
                for word in found_words:
                    sentences = re.split(r'[.!?]+', context)
                    for sentence in sentences:
                        if word in sentence.lower() and len(sentence.strip()) > 10:
                            relevant_parts.append(sentence.strip())
                
                if relevant_parts:
                    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    unique_parts = list(set(relevant_parts))[:2]
                    new_answer = f"–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ {' –∏ '.join(found_words)}: {' '.join(unique_parts)}"
                    
                    print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {new_answer}")
                    return new_answer
                else:
                    new_answer = f"–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ {' –∏ '.join(found_words)}, –Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
                    
                    print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {new_answer}")
                    return new_answer
        
        return answer

    def _extract_main_source_number(self, answer: str) -> int:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –ò–ò"""
        try:
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω [–û–°–ù–û–í–ù–û–ô_–ò–°–¢–û–ß–ù–ò–ö: X] –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞
            import re
            pattern = r'\[–û–°–ù–û–í–ù–û–ô_–ò–°–¢–û–ß–ù–ò–ö:\s*(\d+)\]'
            match = re.search(pattern, answer, re.IGNORECASE)
            
            if match:
                source_number = int(match.group(1))
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–æ–º–µ—Ä –≤ –¥–æ–ø—É—Å—Ç–∏–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (1-5)
                if 1 <= source_number <= 5:
                    return source_number
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –º–∞—Ä–∫–µ—Ä, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 1 (–ø–µ—Ä–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
            return 1
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –Ω–æ–º–µ—Ä–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {e}")
            return 1
    
    def _clean_answer_from_source_marker(self, answer: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –º–∞—Ä–∫–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞"""
        try:
            import re
            # –£–¥–∞–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω [–û–°–ù–û–í–ù–û–ô_–ò–°–¢–û–ß–ù–ò–ö: X] –∏–∑ –æ—Ç–≤–µ—Ç–∞
            pattern = r'\s*\[–û–°–ù–û–í–ù–û–ô_–ò–°–¢–û–ß–ù–ò–ö:\s*\d+\]\s*$'
            cleaned_answer = re.sub(pattern, '', answer, flags=re.IGNORECASE)
            return cleaned_answer.strip()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–∞—Ä–∫–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {e}")
            return answer

    def _reorder_sources_by_main_source(self, sources: List[Dict[str, Any]], main_source_number: int) -> List[Dict[str, Any]]:
        """–ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –ø–æ–º–µ—â–∞—è –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞ –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ."""
        if not sources:
            return []
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Å–ø–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        reordered_sources = sources.copy()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–æ–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –¥–æ–ø—É—Å—Ç–∏–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        if main_source_number < 1 or main_source_number > len(reordered_sources):
            print(f"–ù–æ–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {main_source_number} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
            return reordered_sources
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (main_source_number - 1, —Ç–∞–∫ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 0)
        main_source_index = main_source_number - 1
        
        if main_source_index < len(reordered_sources):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞ –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ
            main_source = reordered_sources.pop(main_source_index)
            reordered_sources.insert(0, main_source)
            
            print(f"–û—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ {main_source_number} –ø–µ—Ä–µ–º–µ—â–µ–Ω –Ω–∞ –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ: {main_source.get('file_name', 'unknown')}")
        
        return reordered_sources

    def _analyze_answer_for_main_source(self, answer: str, sources: List[Dict[str, Any]]) -> int:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ò–ò –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        if not answer or not sources:
            return 1
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        answer_lower = answer.lower()
        
        # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ñ—Ä–∞–∑ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        best_match_index = 0
        best_match_score = 0
        
        for i, source in enumerate(sources):
            source_content = source.get('chunk_content', '').lower()
            if not source_content:
                continue
            
            score = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
            if '‚Äî' in source_content and '‚Äî' in answer_lower:
                # –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–¢–ú–¶ ‚Äî —Ç–æ–≤–∞—Ä–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏"
                definition_pattern = r'([–∞-—è—ë]+)\s*‚Äî\s*([^;]+)'
                import re
                source_definitions = re.findall(definition_pattern, source_content)
                answer_definitions = re.findall(definition_pattern, answer_lower)
                
                for source_def in source_definitions:
                    for answer_def in answer_definitions:
                        if source_def[0] == answer_def[0]:  # –¢–µ—Ä–º–∏–Ω —Å–æ–≤–ø–∞–¥–∞–µ—Ç
                            score += 200  # –í—ã—Å–æ–∫–∏–π –±–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
            source_words = source_content.split()
            answer_words = answer_lower.split()
            
            # –°—á–∏—Ç–∞–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
            common_words = set(source_words) & set(answer_words)
            score += len(common_words) * 5
            
            # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ñ—Ä–∞–∑—ã
            for j in range(len(source_words) - 2):
                for k in range(len(answer_words) - 2):
                    source_phrase = ' '.join(source_words[j:j+3])
                    answer_phrase = ' '.join(answer_words[k:k+3])
                    if source_phrase == answer_phrase and len(source_phrase) > 10:
                        score += 50
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç–µ
            if any(word in answer_lower for word in ['‚Äî', '–æ–∑–Ω–∞—á–∞–µ—Ç', '—ç—Ç–æ', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ']):
                if any(word in source_content for word in ['‚Äî', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—Ç–µ—Ä–º–∏–Ω—ã']):
                    score += 100
            
            # –ë–æ–Ω—É—Å –∑–∞ —Ç–µ—Ä–º–∏–Ω—ã –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
            if '—Ç–µ—Ä–º–∏–Ω—ã –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è' in source_content.lower():
                score += 80
            
            if score > best_match_score:
                best_match_score = score
                best_match_index = i
        
        print(f"RAG: –ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞ - –≤—ã–±—Ä–∞–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ {best_match_index + 1} —Å –±–∞–ª–ª–æ–º {best_match_score}")
        
        return best_match_index + 1  # +1 –ø–æ—Ç–æ–º—É —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 1

    def _presort_sources_by_semantic_importance(self, sources: List[Dict[str, Any]], question: str) -> List[Dict[str, Any]]:
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞"""
        if not sources or not question:
            return sources
        
        question_lower = question.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞
        is_definition_question = any(word in question_lower for word in ['—á—Ç–æ —Ç–∞–∫–æ–µ', '—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '–æ–∑–Ω–∞—á–∞–µ—Ç'])
        is_procedure_question = any(word in question_lower for word in ['–∫–∞–∫', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '–ø—Ä–æ—Ü–µ—Å—Å', '—à–∞–≥–∏', '—ç—Ç–∞–ø—ã'])
        is_requirement_question = any(word in question_lower for word in ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–ø—Ä–∞–≤–∏–ª–∞', '—É—Å–ª–æ–≤–∏—è', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ'])
        
        def calculate_semantic_score(source):
            content = source.get('chunk_content', '').lower()
            score = 0
            
            # –ë–æ–Ω—É—Å –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            if is_definition_question:
                if any(word in content for word in ['‚Äî', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—ç—Ç–æ']):
                    score += 100
                if '—Ç–µ—Ä–º–∏–Ω—ã –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è' in content:
                    score += 50
            
            # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
            if is_procedure_question:
                if any(word in content for word in ['—à–∞–≥', '—ç—Ç–∞–ø', '—Å–Ω–∞—á–∞–ª–∞', '–∑–∞—Ç–µ–º', '–ø–æ—Å–ª–µ']):
                    score += 80
                if any(word in content for word in ['–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '–ø—Ä–æ—Ü–µ—Å—Å', '–∞–ª–≥–æ—Ä–∏—Ç–º']):
                    score += 60
            
            # –ë–æ–Ω—É—Å –∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            if is_requirement_question:
                if any(word in content for word in ['—Ç—Ä–µ–±—É–µ—Ç—Å—è', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–¥–æ–ª–∂–µ–Ω']):
                    score += 70
                if any(word in content for word in ['–ø—Ä–∞–≤–∏–ª–∞', '—É—Å–ª–æ–≤–∏—è', '–∫—Ä–∏—Ç–µ—Ä–∏–∏']):
                    score += 50
            
            # –ë–∞–∑–æ–≤—ã–π –±–æ–Ω—É—Å –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            score += source.get('similarity_score', 0) * 10
            
            return score
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_sources = sorted(sources, key=calculate_semantic_score, reverse=True)
        
        print(f"RAG: –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question[:50]}...'")
        for i, source in enumerate(sorted_sources):
            print(f"  {i+1}. {source.get('file_name', 'unknown')} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {source.get('similarity_score', 0):.3f})")
        
        return sorted_sources

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
yandex_rag_service = YandexRAGService()